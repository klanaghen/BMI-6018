{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science â€“ Homework 6\n",
    "*COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/*\n",
    "\n",
    "Due: Friday, Feburary 28 2025, 11:59pm.\n",
    "\n",
    "In Part 1 of this homework you will scrape github repositories and organize the information in a Pandas dataframe. In Part 2, you will use linear regression to gain meaningful insights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Data\n",
    "First Name: Kim\n",
    "<br>\n",
    "Last Name: Lanaghen\n",
    "<br>\n",
    "E-mail: kim.lanaghen@utah.edu\n",
    "<br>\n",
    "UID: u1210825\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and setup \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline  \n",
    "plt.rcParams['figure.figsize'] = (10, 6) \n",
    "# where the data is stored\n",
    "DATA_PATH = \"/Users/kimlanaghen/Downloads/2026-datascience-homework-main/HW6/snapshots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 files in DATA_PATH\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(DATA_PATH)), \"files in DATA_PATH\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Scrape Github Repository List using BeautifulSoup\n",
    "In this part you will explore Github repositories, specifically the 100 most-starred repositories. You are going to scrape data from a snapshot of [this repository list](https://github.com/search?o=desc&q=stars%3A%3E1&s=stars&type=Repositories)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Check whether you are permitted to scrape the data\n",
    "Before you start to scrape any website you should go through the terms of service and policy documents of the website. Almost all websites post conditions to use their data. Check the terms of [https://github.com/](https://github.com/) (see the tiny \"terms\" link at the bottom of the page) to see whether the site permits you to scrape their data or not. Are you sure you are allowed to scrape?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your solution:**\n",
    "\n",
    "You are allowed to scrape under the conditions that it is not excessive, used for harm, spamming purposes, or to gain access to somebody's personal information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference solution: The [terms of service](https://help.github.com/articles/github-terms-of-service/) do not mention scraping, but the [help pages on the site policy](https://help.github.com/en/github/site-policy/github-acceptable-use-policies#5-scraping-and-api-usage-restrictionsyou) allows scraping. You can scrape Github under the following conditions:\n",
    "\n",
    "- Researchers may scrape public, non-personal information from GitHub for research purposes, only if any publications resulting from that research are open access.\n",
    "- Archivists may scrape GitHub for public data for archival purposes.\n",
    "- You may not scrape GitHub for spamming purposes, including for the purposes of selling GitHub users' personal information, such as to recruiters, headhunters, and job boards.\n",
    "\n",
    "The [robots.txt](https://github.com/robots.txt) is a little less explicit about what is allowed and what not, but overall, since we are scraping Github pages for education/research purposes and not publishing the results, it is reasonable to assume that this is ok to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 Load the Data\n",
    "\n",
    "To avoid any problems with GitHub blocking us from downloading the data many times, we have downloaded and saved a snapshot of the html files for you in the [snapshots](snapshots) folder. Note that the snapshots folder is not completely consistent with what you see on the web â€“ we've made a few patches to the data that makes your task here easier and this data represents a snapshot in time. You will be treating the data folder as your website to be scraped. The path to data folder is stored in `DATA_PATH` variable.\n",
    "\n",
    "In the data folder you will find first 10 pages of highly starred repositories saved as `search_page_1.html`,`search_page_2.html`,`search_page_3.html` ... `search_page_10.html`\n",
    "\n",
    "Check out page 5 if you want to see what happens if you scrape too quickly ðŸ˜‰. **Tip**: you should skip page 5.\n",
    "\n",
    "Now read these html files in python and create a soup object. This is a two step process:\n",
    " * Read the text in the html files\n",
    " * Create the soup from the files that you've read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [1, 2, 3, 4, 6, 7, 8, 9, 10]\n",
    "\n",
    "html_pages = []\n",
    "soups = []\n",
    "  \n",
    "for page in pages:\n",
    "    filename = f\"search_page_{page}.html\"\n",
    "    file_path = os.path.join(DATA_PATH, filename)\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        html = f.read()\n",
    "        html_pages.append(html)\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        soups.append(soup)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "page1_path = os.path.join(DATA_PATH, \"search_page_1.html\")\n",
    "\n",
    "with open(page1_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    \n",
    "    html = f.read()\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Data\n",
    "\n",
    "Extract the following data for each repository, and create a Pandas Dataframe with a row for each repository and a column for each of these datums. \n",
    "\n",
    "+ The name of the repository\n",
    "+ The primary language (there are multiple or none, if multiple, use the first one, if none, use \"none\")\n",
    "+ The number of watching\n",
    "+ The number of stars\n",
    "+ The number of forks\n",
    "+ The number of issues\n",
    "+ Number of commits\n",
    "+ Number of pull requests, and\n",
    "\n",
    "Here's an example for one repository, `freeCodeCamp/freeCodeCamp,` in our dataset: \n",
    "```python\n",
    "{'name': 'freeCodeCamp',\n",
    "'language': 'TypeScript',\n",
    "'watching': '8500',\n",
    "'stars': '410251',\n",
    "'forks': '39007',\n",
    "'issues': 168,\n",
    "'commits': 37591,\n",
    "'pull_requests':66\n",
    "}\n",
    "```\n",
    "### Task 1.3 Extract repository URLs\n",
    "\n",
    "If you look at the results of the 100 most-starred repositories [(this list)](https://github.com/search?o=desc&q=stars%3A%3E1&s=stars&type=Repositories), you will notice that all the information we want to extract for each repository is not in that list. This information is in the repositoryâ€™s individual web page, for example [996icu](https://github.com/996icu/996.ICU). \n",
    "\n",
    "Therefore, you will first have to extract links of each repository from the soup you scraped earlier. When you extract the link for the repository, it will be a path to the stored HTML page for the repository. You will use this path to read the file and extract the above information.\n",
    "\n",
    "Refer to the scraping lecture for details on how to do this. We recommend you use the web inspector to identify the relevant structures.\n",
    "\n",
    "Example of a link that you need to extract - `996icu/996.ICU.html`. This means in the next task you need to access local folder `snapshots/996icu/996.ICU.html`. Similarly, for `521xueweihan/HelloGitHub.html` you should access `snapshots/521xueweihan/HelloGitHub.html` \n",
    "\n",
    "You may need to do string operations to get the desired format for the link. For example, if you get `raw_link = https://github.com/996icu/996.ICU`, you can do\n",
    "`link = raw_link.replace(\"https://github.com/\", \"\") + \".html\"` so you get `996icu/996.ICU.html`.\n",
    "\n",
    "Please title your output 'repo_list', and print this list once you have created it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "repo_list count: 0\n"
     ]
    }
   ],
   "source": [
    "repo_list = []\n",
    "\n",
    "for a in soup.find_all(\"a\"):\n",
    "    href = a.get(\"href\")\n",
    "    if href is None:\n",
    "        continue\n",
    "\n",
    "    # keep only repo-ish links like \"/owner/repo\"\n",
    "    if href.startswith(\"/\") and href.count(\"/\") == 2:\n",
    "        path = href[1:]  # remove leading \"/\"\n",
    "        owner = path.split(\"/\")[0]\n",
    "\n",
    "        # filter out obvious non-repo sections\n",
    "        if owner in [\"search\", \"topics\", \"settings\", \"login\", \"signup\", \"orgs\", \"features\"]:\n",
    "            continue\n",
    "\n",
    "        file_path = path + \".html\"\n",
    "        if file_path not in repo_list:\n",
    "            repo_list.append(file_path)\n",
    "\n",
    "print(repo_list[:10])\n",
    "print(\"repo_list count:\", len(repo_list))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4 Extracting required information\n",
    "\n",
    "Once you have extracted links for each repository, you can start parsing those HTML pages using BeautifulSoup and extract all the required information.\n",
    "\n",
    "**Note**: There are few repositories which do not contain 'issues' field (such as 996icu/996.ICU.html). Therefore, write your code such that it handles this condition as well.\n",
    "\n",
    "**Save the dataframe you created to a new file project_info.csv and include this in your submission.** This separate file will also be graded and is required to earn points.\n",
    "\n",
    "You also need to make sure that you reformat all numerical columns to be integer data. You can do that either as you parse, or when you have a dataframe with strings.\n",
    "\n",
    "Some repositories (~30) are missing in the collection, we have provided code to skip these cases, and similarly in the next frame to NOT include the None numbers in the storage.\n",
    "\n",
    "**Tips**: the exact value of stars and forks can be found on top right corner, with mouse hover over the value. E.g., hover over 410k, shows 410,246. For *watching*, the data is abbreviated, You need to manually convert it. For example, 8.5k should be converted to 8500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def extract_repository_details(url):\n",
    "    row = []\n",
    "    \n",
    "    file_path = Path(\"snapshots\") / url\n",
    "    if file_path.exists():\n",
    "        with file_path.open('r', encoding=\"utf8\") as f:\n",
    "            file = f.read()\n",
    "            \n",
    "    ## Your code goes here\n",
    "    \n",
    "    data = {\"name\": repo_name,\n",
    "            \"language\":language,\n",
    "            \"watching\": watching, \n",
    "            \"stars\": stars, \n",
    "            \"forks\": forks, \n",
    "            \"issues\":issues,\n",
    "            \"commits\": commits,\n",
    "            \"pull_requests\": pull_requests\n",
    "    }\n",
    "        \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## complete extract_repository_details() before running this snippet\n",
    "repo_info_list = []\n",
    "for repo in repo_list:\n",
    "    item = extract_repository_details(repo)\n",
    "    if item is not None:  \n",
    "        repo_info_list.append(item)\n",
    "\n",
    "project_info = pd.DataFrame(repo_info_list)\n",
    "project_info.to_csv('project_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analyzing the repository data\n",
    "\n",
    "In this part, you will analyze the data collected in Part 1 using regression tools. The goal is to identify properties that make a repository popular. \n",
    "\n",
    "First, load the `project_info.csv` file in again. **We need you to do this so that we can run your code below without having to run your scraping code, which can be slow.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_info = pd.read_csv('project_info.csv')\n",
    "project_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1.1 Describe the data\n",
    "\n",
    "+ Get an overview of the data using the describe function.\n",
    "+ Compute the correlation matrix, visualize it with a labeled heat map.\n",
    "+ Interpret what you see, and discuss why some variables may or may not be correlated with others.\n",
    "\n",
    "You can re-use code from your previous homework here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here - describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here - correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here - heat map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1.2 Scatterplot\n",
    "+ Visualize the correlations by making a scatterplot matrix.\n",
    "+ Interpret what you see. Compare this to the correlation matrix. Do either provide you with insight that the other does not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 Train/Test\n",
    "+ Randomly partition the dataset into two groups, train and test, with an 80/20 split. Store these datasets, and use them for the remainder of the assignment. When you train a model, do so on the train set. When you evaluate a model, do so on the test set.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Linear regression\n",
    "\n",
    "+ Use linear regression to try to predict the number of Stars based on Forks, Pull Requests, and Commits. Discuss the R-squared , F-statistic p-value, MSE, and coefficient  p-values seperately for the train set AND R-squared, MSE for the test set. \n",
    "+ Interpret your results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Linear Regression Exploration\n",
    "+ Develop a model which is simpler AND a model which is more complex than in 2.3.1, with the aim of finding a model which performs better on the test set. Hint: refer to the correlation matrix.\n",
    "+ Explain why your chosen model is better than the model in 2.3.1, explain your decision-making process for generating the models, and interpret your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Ridge Regression\n",
    "Refer to section 6.2.1 of [ISL 2015](https://hastie.su.domains/ISLR2/ISLRv2_corrected_June_2023.pdf.download.html) for a description of ridge regression.\n",
    "+ Implement ridge regression on both the variables for 2.3.1, and for your best solution from 2.3.2. \n",
    "+ Plot $\\lambda$ (trained on the train set) against MSE (evaluated on the test set) in order to find an approximately optimal value. \n",
    "+ Explain your selection for $\\lambda$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Lasso Regression\n",
    "Refer to section 6.2.2 of [ISL 2015](https://hastie.su.domains/ISLR2/ISLRv2_corrected_June_2023.pdf.download.html) for a description of lasso regression.\n",
    "+ Implement lasso regression on both the variables for 2.3.1, and for your best solution from 2.3.2. \n",
    "+ Plot $\\lambda$ (trained on the train set) against MSE (evaluated on the test set) in order to find an approximately optimal value. \n",
    "+ Explain your selection for $\\lambda$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Regression Methods Analysis\n",
    "Compare the results of each regression method for this use case. Which one performed the best, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Regression Methods Study\n",
    "Based on your reading of the textbook and the prior exercises, explain the differences between linear, lasso, and ridge regression, and when you would want to use each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation:** TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
